{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c127f77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MSE: 1.15\n",
      "Test R^2: 1.00\n"
     ]
    }
   ],
   "source": [
    "# Build a regression model to predict SOC using time, voltage, current, and max_temperature\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"..\\\\unibo-powertools-dataset\\\\unibo-powertools-dataset\\\\test_result_trial_end_cleaned_v1.0.csv\")\n",
    "\n",
    "# Select features and target\n",
    "features = ['time', 'voltage', 'current', 'max_temperature']\n",
    "target = 'SOC'\n",
    "\n",
    "# Drop rows with missing values in selected columns\n",
    "model_df = df.dropna(subset=features + [target])\n",
    "\n",
    "X = model_df[features]\n",
    "y = model_df[target]\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train model\n",
    "model = RandomForestRegressor(n_estimators=20, random_state=42, n_jobs=-1)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(f\"Test MSE: {mse:.2f}\")\n",
    "print(f\"Test R^2: {r2:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4670395d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved as: ../models/SOC_RandomForestRegressor_v1.0_test_result_trial_end_v1.0_20250804_003236.joblib\n"
     ]
    }
   ],
   "source": [
    "# Save the trained model with a unique name including version info\n",
    "import joblib\n",
    "import datetime\n",
    "\n",
    "# Define model and dataset version info\n",
    "model_version = \"v1.0\"\n",
    "dataset_version = \"test_result_trial_end_v1.0\"\n",
    "model_type = \"RandomForestRegressor\"\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "model_filename = f\"../models/SOC_{model_type}_{model_version}_{dataset_version}_{timestamp}.joblib\"\n",
    "joblib.dump(model, model_filename)\n",
    "print(f\"Model saved as: {model_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e98f0c40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to: ../results\\model_results.csv\n",
      "\n",
      "Current Model Performance:\n",
      "Model: RandomForestRegressor\n",
      "Dataset: test_result_trial_end_v1.0\n",
      "Features: time, voltage, current, max_temperature\n",
      "Test Size: 81153\n",
      "MSE: 1.1479\n",
      "RMSE: 1.0714\n",
      "MAE: 0.2397\n",
      "R² Score: 0.9968\n",
      "MAPE: 12.05%\n",
      "\n",
      "=== All Model Results ===\n",
      "              model_name                dataset_name  r2_score    rmse  \\\n",
      "0  RandomForestRegressor  test_result_trial_end_v1.0    0.9968  1.0714   \n",
      "\n",
      "      mae    mape  \n",
      "0  0.2397  12.048  \n",
      "\n",
      "=== Feature Importance ===\n",
      "           feature  importance\n",
      "1          voltage    0.893856\n",
      "2          current    0.047865\n",
      "0             time    0.047764\n",
      "3  max_temperature    0.010514\n"
     ]
    }
   ],
   "source": [
    "# Create comprehensive results tracking system\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "\n",
    "# Create results directory if it doesn't exist\n",
    "results_dir = '../results'\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "# Calculate comprehensive metrics\n",
    "def calculate_metrics(y_true, y_pred, model_name, dataset_name, features_used):\n",
    "    \"\"\"Calculate comprehensive evaluation metrics\"\"\"\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    \n",
    "    # Additional metrics\n",
    "    mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100  # Mean Absolute Percentage Error\n",
    "    \n",
    "    return {\n",
    "        'model_name': model_name,\n",
    "        'dataset_name': dataset_name,\n",
    "        'features_used': ', '.join(features_used),\n",
    "        'num_features': len(features_used),\n",
    "        'train_size': len(y_train),\n",
    "        'test_size': len(y_true),\n",
    "        'mse': mse,\n",
    "        'rmse': rmse,\n",
    "        'mae': mae,\n",
    "        'r2_score': r2,\n",
    "        'mape': mape,\n",
    "        'timestamp': timestamp\n",
    "    }\n",
    "\n",
    "# Calculate metrics for current model\n",
    "current_metrics = calculate_metrics(\n",
    "    y_test, y_pred, \n",
    "    model_type, \n",
    "    dataset_version, \n",
    "    features\n",
    ")\n",
    "\n",
    "# Load existing results or create new DataFrame\n",
    "results_file = os.path.join(results_dir, 'model_results.csv')\n",
    "if os.path.exists(results_file):\n",
    "    results_df = pd.read_csv(results_file)\n",
    "else:\n",
    "    results_df = pd.DataFrame()\n",
    "\n",
    "# Add current results\n",
    "new_result = pd.DataFrame([current_metrics])\n",
    "results_df = pd.concat([results_df, new_result], ignore_index=True)\n",
    "\n",
    "# Save updated results\n",
    "results_df.to_csv(results_file, index=False)\n",
    "\n",
    "print(f\"Results saved to: {results_file}\")\n",
    "print(\"\\nCurrent Model Performance:\")\n",
    "print(f\"Model: {current_metrics['model_name']}\")\n",
    "print(f\"Dataset: {current_metrics['dataset_name']}\")\n",
    "print(f\"Features: {current_metrics['features_used']}\")\n",
    "print(f\"Test Size: {current_metrics['test_size']}\")\n",
    "print(f\"MSE: {current_metrics['mse']:.4f}\")\n",
    "print(f\"RMSE: {current_metrics['rmse']:.4f}\")\n",
    "print(f\"MAE: {current_metrics['mae']:.4f}\")\n",
    "print(f\"R² Score: {current_metrics['r2_score']:.4f}\")\n",
    "print(f\"MAPE: {current_metrics['mape']:.2f}%\")\n",
    "\n",
    "# Display all results\n",
    "print(\"\\n=== All Model Results ===\")\n",
    "if len(results_df) > 0:\n",
    "    print(results_df[['model_name', 'dataset_name', 'r2_score', 'rmse', 'mae', 'mape']].round(4))\n",
    "else:\n",
    "    print(\"No previous results found.\")\n",
    "\n",
    "# Save detailed results with feature importance if available\n",
    "if hasattr(model, 'feature_importances_'):\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': features,\n",
    "        'importance': model.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    print(\"\\n=== Feature Importance ===\")\n",
    "    print(feature_importance)\n",
    "    \n",
    "    # Save feature importance\n",
    "    importance_file = os.path.join(results_dir, f'feature_importance_{model_type}_{timestamp}.csv')\n",
    "    feature_importance.to_csv(importance_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6b1348fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training LinearRegression...\n",
      "Model saved: ../models/SOC_LinearRegression_v1.0_test_result_trial_end_v1.0_20250804_004225.joblib\n",
      "R² Score: 0.8599\n",
      "RMSE: 7.0773\n",
      "\n",
      "Training DecisionTreeRegressor...\n",
      "Model saved: ../models/SOC_DecisionTreeRegressor_v1.0_test_result_trial_end_v1.0_20250804_004228.joblib\n",
      "R² Score: 0.9948\n",
      "RMSE: 1.3635\n",
      "\n",
      "Training GradientBoostingRegressor...\n",
      "Model saved: ../models/SOC_GradientBoostingRegressor_v1.0_test_result_trial_end_v1.0_20250804_004300.joblib\n",
      "R² Score: 0.9480\n",
      "RMSE: 4.3139\n",
      "\n",
      "All results updated in: ../results\\model_results.csv\n",
      "\n",
      "=== Final Model Comparison ===\n",
      "               model_name  r2_score   rmse    mae      mape\n",
      "    RandomForestRegressor    0.9968 1.0714 0.2397   12.0480\n",
      "         LinearRegression    0.8599 7.0773 5.4789 1467.9599\n",
      "    DecisionTreeRegressor    0.9948 1.3635 0.2473   12.4251\n",
      "GradientBoostingRegressor    0.9480 4.3139 3.2723  191.8751\n"
     ]
    }
   ],
   "source": [
    "# Example: Train different models for comparison\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "import datetime\n",
    "\n",
    "# Test different models\n",
    "models_to_test = {\n",
    "    'LinearRegression': LinearRegression(),\n",
    "    'DecisionTreeRegressor': DecisionTreeRegressor(random_state=42),\n",
    "    'GradientBoostingRegressor': GradientBoostingRegressor(random_state=42, n_estimators=50)\n",
    "}\n",
    "\n",
    "# Train and evaluate each model\n",
    "for model_name, model_instance in models_to_test.items():\n",
    "    print(f\"\\nTraining {model_name}...\")\n",
    "    \n",
    "    # Train the model\n",
    "    model_instance.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred_new = model_instance.predict(X_test)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    timestamp_new = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    metrics = calculate_metrics(\n",
    "        y_test, y_pred_new, \n",
    "        model_name, \n",
    "        dataset_version, \n",
    "        features\n",
    "    )\n",
    "    \n",
    "    # Update timestamp for this model\n",
    "    metrics['timestamp'] = timestamp_new\n",
    "    \n",
    "    # Add to results\n",
    "    new_result = pd.DataFrame([metrics])\n",
    "    results_df = pd.concat([results_df, new_result], ignore_index=True)\n",
    "    \n",
    "    # Save the model\n",
    "    model_filename_new = f\"../models/SOC_{model_name}_v1.0_{dataset_version}_{timestamp_new}.joblib\"\n",
    "    joblib.dump(model_instance, model_filename_new)\n",
    "    \n",
    "    print(f\"Model saved: {model_filename_new}\")\n",
    "    print(f\"R² Score: {metrics['r2_score']:.4f}\")\n",
    "    print(f\"RMSE: {metrics['rmse']:.4f}\")\n",
    "\n",
    "# Save updated results\n",
    "results_df.to_csv(results_file, index=False)\n",
    "print(f\"\\nAll results updated in: {results_file}\")\n",
    "\n",
    "# Display final comparison\n",
    "print(\"\\n=== Final Model Comparison ===\")\n",
    "comparison_cols = ['model_name', 'r2_score', 'rmse', 'mae', 'mape']\n",
    "print(results_df[comparison_cols].round(4).to_string(index=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
